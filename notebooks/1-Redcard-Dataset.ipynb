{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redcard Exploratory Data Analysis\n",
    "\n",
    "This dataset is taken from a fantastic paper that looks to see how analytical choices made by different data science teams on the same dataset in an attempt to answer the same research question affect the final outcome.\n",
    "\n",
    "[Many analysts, one dataset: Making transparent how variations in analytical choices affect results](https://osf.io/gvm2z/)\n",
    "\n",
    "The data can be found [here](https://osf.io/47tnc/).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## The Task\n",
    "\n",
    "Do an Exploratory Data Analysis on the redcard dataset. Keeping in mind the question is the following: **Are soccer referees more likely to give red cards to dark-skin-toned players than light-skin-toned players?**\n",
    "\n",
    "- Before plotting/joining/doing something, have a question or hypothesis that you want to investigate\n",
    "- Draw a plot of what you want to see on paper to sketch the idea\n",
    "- Write it down, then make the plan on how to get there\n",
    "- How do you know you aren't fooling yourself\n",
    "- What else can I check if this is actually true?\n",
    "- What evidence could there be that it's wrong?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "from __future__ import absolute_import, division, print_function\n",
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.pyplot import GridSpec\n",
    "import seaborn as sns\n",
    "import mpld3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, sys\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_context(\"poster\", font_scale=1.3)\n",
    "\n",
    "import missingno as msno\n",
    "import pandas_profiling\n",
    "\n",
    "import hdbscan\n",
    "from sklearn.datasets import make_blobs\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About the Data\n",
    "\n",
    "> The dataset is available as a list with 146,028 dyads of players and referees and includes details from players, details from referees and details regarding the interactions of player-referees. A summary of the variables of interest can be seen below. A detailed description of all variables included can be seen in the README file on the project website. \n",
    "\n",
    "> From a company for sports statistics, we obtained data and profile photos from all soccer players (N = 2,053) playing in the first male divisions of England, Germany, France and Spain in the 2012-2013 season and all referees (N = 3,147) that these players played under in their professional career (see Figure 1). We created a dataset of player–referee dyads including the number of matches players and referees encountered each other and our dependent variable, the number of red cards given to a player by a particular referee throughout all matches the two encountered each other.\n",
    "\n",
    "> -- https://docs.google.com/document/d/1uCF5wmbcL90qvrk_J27fWAvDcDNrO9o_APkicwRkOKc/edit\n",
    "\n",
    "\n",
    "| Variable Name: | Variable Description: | \n",
    "| -- | -- | \n",
    "| playerShort | short player ID | \n",
    "| player | player name | \n",
    "| club | player club | \n",
    "| leagueCountry | country of player club (England, Germany, France, and Spain) | \n",
    "| height | player height (in cm) | \n",
    "| weight | player weight (in kg) | \n",
    "| position | player position | \n",
    "| games | number of games in the player-referee dyad | \n",
    "| goals | number of goals in the player-referee dyad | \n",
    "| yellowCards | number of yellow cards player received from the referee | \n",
    "| yellowReds | number of yellow-red cards player received from the referee | \n",
    "| redCards | number of red cards player received from the referee | \n",
    "| photoID | ID of player photo (if available) | \n",
    "| rater1 | skin rating of photo by rater 1 | \n",
    "| rater2 | skin rating of photo by rater 2 | \n",
    "| refNum | unique referee ID number (referee name removed for anonymizing purposes) | \n",
    "| refCountry | unique referee country ID number | \n",
    "| meanIAT | mean implicit bias score (using the race IAT) for referee country | \n",
    "| nIAT | sample size for race IAT in that particular country | \n",
    "| seIAT | standard error for mean estimate of race IAT   | \n",
    "| meanExp | mean explicit bias score (using a racial thermometer task) for referee country | \n",
    "| nExp | sample size for explicit bias in that particular country | \n",
    "| seExp |  standard error for mean estimate of explicit bias measure | \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Uncomment one of the following lines and run the cell:\n",
    "\n",
    "# df = pd.read_csv(\"../data/redcard/redcard.csv.gz\", compression='gzip')\n",
    "# df = pd.read_csv(\"https://github.com/cmawer/pycon-2017-eda-tutorial/raw/master/data/redcard/redcard.csv.gz\", compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_columns = df.columns.tolist()\n",
    "all_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What the teams found\n",
    "\n",
    "\n",
    "### Choices in model features\n",
    "\n",
    "The following is the covariates chosen for the respective models: \n",
    "\n",
    "<img src=\"figures/covariates.png\" width=80%;>\n",
    "\n",
    "\n",
    "### Choices in modeling\n",
    "\n",
    "Of the many choices made by the team, here is a small selection of the models used to answer this question:\n",
    "\n",
    "\n",
    "<img src=\"figures/models.png\" width=80%;>\n",
    "\n",
    "\n",
    "## Final Results\n",
    "\n",
    " - 0 teams: negative effect\n",
    " - 9 teams: no significant relationship\n",
    " - 20 teams: finding a positive effect\n",
    "\n",
    "<img src=\"figures/results.png\" width=80%;>\n",
    "\n",
    "Above image from: http://fivethirtyeight.com/features/science-isnt-broken/#part2\n",
    "\n",
    "\n",
    "> …selecting randomly from the present teams, there would have been a 69% probability of reporting a positive result and a 31% probability of reporting a null effect. This raises the possibility that many research projects contain hidden uncertainty due to the wide range of analytic choices available to the researchers. -- Silberzahn, R., Uhlmann, E. L., Martin, D. P., Pasquale, Aust, F., Awtrey, E. C., … Nosek, B. A. (2015, August 20). Many analysts, one dataset: Making transparent how variations in analytical choices affect results. Retrieved from osf.io/gvm2z\n",
    "\n",
    "\n",
    "Images and data from: Silberzahn, R., Uhlmann, E. L., Martin, D. P., Pasquale, Aust, F., Awtrey, E. C., … Nosek, B. A. (2015, August 20). Many analysts, one dataset: Making transparent how variations in analytical choices affect results. Retrieved from osf.io/gvm2z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge\n",
    "\n",
    "Before looking below, try to answer some high level questions about the dataset. \n",
    "\n",
    "\n",
    "How do we operationalize the question of referees giving more red cards to dark skinned players?\n",
    "* Counterfactual: if the player were lighter, a ref is more likely to have given a yellow or no card **for the same offense under the same conditions**\n",
    "* Regression: accounting for confounding, darker players have positive coefficient on regression against proportion red/total card\n",
    "\n",
    "Potential issues\n",
    "* How to combine rater1 and rater2? Average them? What if they disagree? Throw it out?\n",
    "* Is data imbalanced, i.e. red cards are very rare?\n",
    "* Is data biased, i.e. players have different amounts of play time? Is this a summary of their whole career?\n",
    "* How do I know I've accounted for all forms of confounding?\n",
    "\n",
    "**First, is there systematic discrimination across all refs?**\n",
    "\n",
    "Exploration/hypotheses:\n",
    "* Distribution of games played\n",
    "* red cards vs games played\n",
    "* Reds per game played vs total cards per game played by skin color\n",
    "* Distribution of # red, # yellow, total cards, and fraction red per game played for all players by avg skin color\n",
    "* How many refs did players encounter?\n",
    "* Do some clubs play more aggresively and get carded more? Or are more reserved and get less?\n",
    "* Does carding vary by leagueCountry?\n",
    "* Do high scorers get more slack (fewer cards) for the same position?\n",
    "* Are there some referees that give more red/yellow cards than others?\n",
    "* how consistent are raters? Check with Cohen's kappa.\n",
    "* how do red cards vary by position? e.g. defenders get more?\n",
    "* Do players with more games get more cards, and is there difference across skin color?\n",
    "* indication of bias depending on refCountry?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understand how the data's organized\n",
    "\n",
    "The dataset is a single csv where it aggregated every interaction between referee and player into a single row. In other words: Referee A refereed Player B in, say, 10 games, and gave 2 redcards during those 10 games. Then there would be a unique row in the dataset that said: \n",
    "\n",
    "    Referee A, Player B, 2 redcards, ... \n",
    "\n",
    "This has several implications that make this first step to understanding and dealing with this data a bit tricky. First, is that the information about Player B is repeated each time -- meaning if we did a simple average of some metric of we would likely get a misleading result. \n",
    "\n",
    "For example, asking \"what is the average `weight` of the players?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.height.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "np.mean(df.groupby('playerShort').height.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing a simple average over the rows will risk double-counting the same player multiple times, for a skewed average. The simple (incorrect) average is ~76.075 kg, but the average weight of the players is ~75.639 kg. There are multiple ways of doing this, but doing a groupby on player makes it so that so each player gets counted exactly once.\n",
    "\n",
    "Not a huge difference in this case but already an illustration of some difficulty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tidy Data\n",
    "\n",
    "Hadley Wickham's concept of a **tidy dataset** summarized as:\n",
    "\n",
    ">  - Each variable forms a column\n",
    ">  - Each observation forms a row\n",
    ">  - Each type of observational unit forms a table\n",
    "\n",
    "A longer paper describing this can be found in this [pdf](https://www.jstatsoft.org/article/view/v059i10/v59i10.pdf).\n",
    "\n",
    "Having datasets in this form allows for much simpler analyses. So the first step is to try and clean up the dataset into a tidy dataset. \n",
    "\n",
    "The first step that I am going to take is to break up the dataset into the different observational units. By that I'm going to have separate tables (or dataframes) for: \n",
    "\n",
    " - players\n",
    " - clubs\n",
    " - referees\n",
    " - countries\n",
    " - dyads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Tidy Players Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "player_index = 'playerShort'\n",
    "player_cols = [#'player', # drop player name, we have unique identifier\n",
    "               'birthday',\n",
    "               'height',\n",
    "               'weight',\n",
    "               'position',\n",
    "               'photoID',\n",
    "               'rater1',\n",
    "               'rater2',\n",
    "              ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Count the unique variables (if we got different weight values, \n",
    "# for example, then we should get more than one unique value in this groupby)\n",
    "all_cols_unique_players = df.groupby('playerShort').agg({col:'nunique' for col in player_cols})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cols_unique_players.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If all values are the same per player then this should be empty (and it is!)\n",
    "all_cols_unique_players[all_cols_unique_players > 1].dropna().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A slightly more elegant way to test the uniqueness\n",
    "all_cols_unique_players[all_cols_unique_players > 1].dropna().shape[0] == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hooray, our data passed our sanity check. Let's create a function to create a table and run this check for each table that we create."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_subgroup(dataframe, g_index, g_columns):\n",
    "    \"\"\"Helper function that creates a sub-table from the columns and runs a quick uniqueness test.\"\"\"\n",
    "    g = dataframe.groupby(g_index).agg({col:'nunique' for col in g_columns})\n",
    "    if g[g > 1].dropna().shape[0] != 0:\n",
    "        print(\"Warning: you probably assumed this had all unique values but it doesn't.\")\n",
    "    return dataframe.groupby(g_index).agg({col:'max' for col in g_columns})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "players = get_subgroup(df, player_index, player_cols)\n",
    "players.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_subgroup(dataframe, g_index, subgroup_name, prefix='../data/redcard/raw_'):\n",
    "    save_subgroup_filename = \"\".join([prefix, subgroup_name, \".csv.gz\"])\n",
    "    dataframe.to_csv(save_subgroup_filename, compression='gzip')\n",
    "    test_df = pd.read_csv(save_subgroup_filename, compression='gzip', index_col=g_index)\n",
    "    # Test that we recover what we send in\n",
    "    if dataframe.equals(test_df):\n",
    "        print(\"Test-passed: we recover the equivalent subgroup dataframe.\")\n",
    "    else:\n",
    "        print(\"Warning -- equivalence test!!! Double-check.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_subgroup(players, player_index, \"players\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Tidy Clubs Table\n",
    "\n",
    "Create the clubs table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "club_index = 'club'\n",
    "club_cols = ['leagueCountry']\n",
    "clubs = get_subgroup(df, club_index, club_cols)\n",
    "clubs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clubs['leagueCountry'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_subgroup(clubs, club_index, \"clubs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Tidy Referees Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "referee_index = 'refNum'\n",
    "referee_cols = ['refCountry']\n",
    "referees = get_subgroup(df, referee_index, referee_cols)\n",
    "referees.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "referees.refCountry.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_subgroup(referees, referee_index, \"referees\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Tidy Countries Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_index = 'refCountry'\n",
    "country_cols = ['Alpha_3', # rename this name of country\n",
    "                'meanIAT',\n",
    "                'nIAT',\n",
    "                'seIAT',\n",
    "                'meanExp',\n",
    "                'nExp',\n",
    "                'seExp',\n",
    "               ]\n",
    "countries = get_subgroup(df, country_index, country_cols)\n",
    "countries.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rename_columns = {'Alpha_3':'countryName'}\n",
    "countries = countries.rename(columns=rename_columns)\n",
    "countries.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_subgroup(countries, country_index, \"countries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ok testing this out: \n",
    "test_df = pd.read_csv(\"../data/redcard/raw_countries.csv.gz\", compression='gzip', index_col=country_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (_, row1), (_, row2) in zip(test_df.iterrows(), countries.iterrows()):\n",
    "    if not row1.equals(row2):\n",
    "        print(row1)\n",
    "        print()\n",
    "        print(row2)\n",
    "        print()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row1.eq(row2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row1.seIAT - row2.seIAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like precision error, so I'm not concerned. All other sanity checks pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create separate (not yet Tidy) Dyads Table\n",
    "\n",
    "This is one of the more complex tables to reason about -- so we'll save it for a bit later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dyad_index = ['refNum', 'playerShort']\n",
    "dyad_cols = ['games',\n",
    "             'victories',\n",
    "             'ties',\n",
    "             'defeats',\n",
    "             'goals',\n",
    "             'yellowCards',\n",
    "             'yellowReds',\n",
    "             'redCards',\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dyads = get_subgroup(df, g_index=dyad_index, g_columns=dyad_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dyads.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dyads.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dyads[dyads.redCards > 1].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_subgroup(dyads, dyad_index, \"dyads\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "py3",
   "language": "python",
   "name": "py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
